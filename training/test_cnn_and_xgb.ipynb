{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options 1\n",
      "\n",
      "Loading clip: 1326-2023-02-03-17-45-57-8a41f628\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1325-2023-02-03-17-46-35-11ad4b90\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1322-2023-02-03-14-25-41-7883a14f\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1318-2023-02-03-10-41-29-03c72c25\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1317-2023-02-03-09-41-14-30f69b87\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1316-2023-02-03-09-34-09-31ef7938\n",
      "Number of clips: 4\n",
      "\n",
      "Loading clip: 1311-2023-02-02-14-30-40-96be0928\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1312-2023-02-02-16-21-47-cd28e8e0\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1313-2023-02-02-16-31-53-67b1bdbe\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1329-2023-02-06-10-40-29-c4308424\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1332-2023-02-06-11-58-03-8fc75e25\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1333-2023-02-06-13-17-50-ebc1e021\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1336-2023-02-06-15-28-30-adb9dbfb\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1337-2023-02-06-15-36-31-211278ea\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1338-2023-02-06-16-29-00-adceb23d\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1339-2023-02-06-17-24-49-5b24f5ba\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1340-2023-02-06-17-37-00-e6218648\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1341-2023-02-07-10-38-42-d828e5ac\n",
      "Number of clips: 4\n",
      "\n",
      "Loading clip: 1342-2023-02-07-10-43-55-b5709e1f\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1343-2023-02-07-11-37-02-214c1078\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1345-2023-02-07-13-19-29-f5930ed0\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 1344-2023-02-07-13-19-49-d1517542\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 1346-2023-02-07-14-22-59-b1384544\n",
      "Number of clips: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/users/tom/git/neon_blink_detection/\")\n",
    "sys.path.append(\"/users/tom/git/neon_blink_detection/src\")\n",
    "\n",
    "from training.helper import ClassifierParams, Results\n",
    "from src.neon_blink_detector import get_params\n",
    "of_params, pp_params, _ = get_params()\n",
    "import numpy as np\n",
    "from training.helper import get_augmentation_options\n",
    "from src.helper import OfParams, PPParams, AugParams\n",
    "from training.video_loader import video_loader\n",
    "from training.datasets_loader import concatenate_all_samples, concatenate\n",
    "\n",
    "clip_names = np.load(\"/users/tom/git/neon_blink_detection/clip_list.npy\")\n",
    "\n",
    "of_params = OfParams(n_layers=5, layer_interval=7, average=False, img_shape=(64, 64), grid_size=4, step_size=7, window_size=11, stop_steps=3)\n",
    "\n",
    "aug_params_options = get_augmentation_options()\n",
    "aug_params = aug_params_options[0]\n",
    "\n",
    "rec = video_loader(of_params, aug_params)\n",
    "\n",
    "rec.collect(clip_names, bg_ratio=1, augment=False, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/tom/git/neon_blink_detection/training/cnn.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n",
      "/users/tom/git/neon_blink_detection/training/cnn.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Validation loss: 0.26984216694305413\n",
      "Epoch 2/100\n",
      "Validation loss: 0.23062998276666133\n",
      "Epoch 3/100\n",
      "Validation loss: 0.20695850293893014\n",
      "Epoch 4/100\n",
      "Validation loss: 0.1894970672055535\n",
      "Epoch 5/100\n",
      "Validation loss: 0.1795741290017201\n",
      "Epoch 6/100\n",
      "Validation loss: 0.17512992568259692\n",
      "Epoch 7/100\n",
      "Validation loss: 0.17496164998270736\n",
      "Epoch 8/100\n",
      "Validation loss: 0.16099902608832956\n",
      "Epoch 9/100\n",
      "Validation loss: 0.15243457471204064\n",
      "Epoch 10/100\n",
      "Validation loss: 0.15183160753145705\n",
      "Epoch 11/100\n",
      "Validation loss: 0.15173971292691946\n",
      "Epoch 12/100\n",
      "Validation loss: 0.1433104367885929\n",
      "Epoch 13/100\n",
      "Validation loss: 0.143464439011512\n",
      "Epoch 14/100\n",
      "Validation loss: 0.1430923905781042\n",
      "Epoch 15/100\n",
      "Validation loss: 0.141280961936734\n",
      "Epoch 16/100\n",
      "Validation loss: 0.13528128966254038\n",
      "Epoch 17/100\n",
      "Validation loss: 0.1268652409355897\n",
      "Epoch 18/100\n",
      "Validation loss: 0.1266434966674904\n",
      "Epoch 19/100\n",
      "Validation loss: 0.12917113724932835\n",
      "Epoch 20/100\n",
      "Validation loss: 0.12388510059864416\n",
      "Epoch 21/100\n",
      "Validation loss: 0.1348867277703146\n",
      "Epoch 22/100\n",
      "Validation loss: 0.12332034810534577\n",
      "Epoch 23/100\n",
      "Validation loss: 0.12092184616647063\n",
      "Epoch 24/100\n",
      "Validation loss: 0.11660521376171033\n",
      "Epoch 25/100\n",
      "Validation loss: 0.12079299956451367\n",
      "Epoch 26/100\n",
      "Validation loss: 0.12599711562336904\n",
      "Epoch 27/100\n",
      "Validation loss: 0.11637261398408534\n",
      "Epoch 28/100\n",
      "Validation loss: 0.11851913332735209\n",
      "Early stopping after 28 epochs\n"
     ]
    }
   ],
   "source": [
    "from training.cnn import OpticalFlowCNN, OpticalFlowDataset\n",
    "from training.run_one import train_cnn\n",
    "\n",
    "classifier, scores = train_cnn(\n",
    "            rec,\n",
    "            clip_names,\n",
    "            classifier_params=None,\n",
    "            export_path=None,\n",
    "            idx=0,\n",
    "            augment_data=False,\n",
    "            pp_params=pp_params,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = concatenate(rec.all_features, clip_names)\n",
    "samples_gt = concatenate_all_samples(rec.all_samples, clip_names)\n",
    "labels = samples_gt.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/tom/git/neon_blink_detection/training/cnn.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch_features = torch.from_numpy(features).float().cuda()\n",
    "cnn_features = classifier.predict(torch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 50\n",
    "\n",
    "indices = np.arange(0, features.shape[0])\n",
    "all_indices = np.array([np.arange(index - length, index + length) for index in indices])\n",
    "all_indices = np.clip(all_indices, 0, features.shape[0]-1)\n",
    "\n",
    "all_cnn_features = np.array(cnn_features[all_indices, :].reshape(-1, 2*length * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_fit took 1 m 41 s\n"
     ]
    }
   ],
   "source": [
    "# import xgb classifi\n",
    "from functions.classifiers import Classifier as XGB\n",
    "from functions.pipeline import get_classifier_params\n",
    "\n",
    "classifier_params = get_classifier_params()\n",
    "xgb = XGB(classifier_params)\n",
    "xgb.on_fit(features=all_cnn_features, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading clip: 2023-03-01_09-59-07-2ea49126\n",
      "Number of clips: 2\n",
      "\n",
      "Loading clip: 2023-01-27_15-59-54-49a115d5\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 2023-02-01_11-45-11-7621531e\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 2023-01-27_16-10-14-a2a8cbe1\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 2023-01-27_16-15-26-57802f75\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 2023-01-27_16-24-04-eb4305b1\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: 2023-01-27_16-31-52-5f743ed0\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: padel_tennis_neon_01-b922b245\n",
      "Number of clips: 1\n",
      "\n",
      "Loading clip: padel_tennis_neon_03-2ded8f56\n",
      "Number of clips: 1\n"
     ]
    }
   ],
   "source": [
    "clip_names_test = [\n",
    "    \"2023-03-01_09-59-07-2ea49126\",  # kai bike\n",
    "    \"2023-01-27_15-59-54-49a115d5\",  # tom computer\n",
    "    \"2023-02-01_11-45-11-7621531e\",  # kai computer\n",
    "    \"2023-01-27_16-10-14-a2a8cbe1\",  # ryan discussing\n",
    "    \"2023-01-27_16-15-26-57802f75\",  # tom walking\n",
    "    \"2023-01-27_16-24-04-eb4305b1\",  # kai walking\n",
    "    \"2023-01-27_16-31-52-5f743ed0\",  # moritz snowboarding\n",
    "    \"padel_tennis_neon_01-b922b245\",  # mgg padel\n",
    "    \"padel_tennis_neon_03-2ded8f56\",  # mgg partner padel\n",
    "]\n",
    "\n",
    "rec.collect(clip_names_test, bg_ratio=1, augment=False, idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = concatenate(rec.all_features, clip_names_test)\n",
    "samples_gt = concatenate_all_samples(rec.all_samples, clip_names_test)\n",
    "labels = samples_gt.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, features.shape[0])\n",
    "all_indices = np.array([np.arange(index - length, index + length) for index in indices])\n",
    "all_indices = np.clip(all_indices, 0, features.shape[0]-1)\n",
    "\n",
    "torch_features_test = torch.from_numpy(features).float().cuda()\n",
    "cnn_features_test = classifier.predict(torch_features_test)\n",
    "all_cnn_features_test = np.array(cnn_features_test[all_indices, :].reshape(-1, 2 * length * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(xgb.predict(all_cnn_features_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94371758 0.82234178 0.80445375]\n",
      "[0.9752774  0.77070229 0.7344459 ]\n",
      "[0.91413627 0.88139825 0.8892142 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "print(f1_score(labels, pred_labels, average=None))\n",
    "print(recall_score(labels, pred_labels,  average=None))\n",
    "print(precision_score(labels, pred_labels,  average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176021, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.94316394 0.82210183 0.80179752]\n",
    "[0.97496918 0.77304153 0.72975878]\n",
    "[0.91336824 0.87781123 0.88961677]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tom_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
