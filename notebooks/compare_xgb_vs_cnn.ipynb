{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/anaconda3/envs/tom_py310/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/users/tom/git/neon_blink_detection/\")\n",
    "sys.path.append(\"/users/tom/git/neon_blink_detection/src\")\n",
    "\n",
    "# from evaluation_functions import cnn_predictions, xgb_predictions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cnn_path = \"/users/tom/git/neon_blink_detection/training-XGBClassifier-3-200320231657/n_lay5-lay_intv7-grid4-win15-trans0.0-scale0.0-speed0.0/results.pkl\"\n",
    "\n",
    "with open(cnn_path, 'rb') as f:\n",
    "    cnn_results = pickle.load(f)\n",
    "\n",
    "xgb_path = \"/users/tom/git/neon_blink_detection/training-XGBClassifier-3-100320231148/n_lay5-lay_intv7-grid4-win15-trans0.0-scale0.0/results.pkl\"\n",
    "\n",
    "with open(xgb_path, 'rb') as f:\n",
    "    xgb_results = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cnn_results.metrics_sample_val\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "mean_precision_cnn = np.mean([cnn_results.metrics_pp_val.scores_list[x].F1 for x in range(5)])\n",
    "mean_recall_cnn = np.mean([cnn_results.metrics_pp_val.scores_list[x].recall for x in range(5)])\n",
    "mean_f1_score_cnn = np.mean([cnn_results.metrics_pp_val.scores_list[x].precision for x in range(5)])\n",
    "\n",
    "mean_precision_xgb = np.mean([xgb_results.metrics_pp_val.scores_list[x].F1 for x in range(5)])\n",
    "mean_recall_xgb = np.mean([xgb_results.metrics_pp_val.scores_list[x].recall for x in range(5)])\n",
    "mean_f1_score_xgb = np.mean([xgb_results.metrics_pp_val.scores_list[x].precision for x in range(5)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "--------------------\n",
      "Precision:  0.9487285807744603\n",
      "Recall:  0.9711876729965162\n",
      "F1 Score:  0.9275466876311128\n",
      "XGB\n",
      "--------------------\n",
      "Precision:  0.9338402586407414\n",
      "Recall:  0.960949825028932\n",
      "F1 Score:  0.9087036289905799\n"
     ]
    }
   ],
   "source": [
    "# print results in a visally appealing way\n",
    "print(\"CNN\")\n",
    "print(\"--------------------\")\n",
    "print(\"Precision: \", mean_precision_cnn)\n",
    "print(\"Recall: \", mean_recall_cnn)\n",
    "print(\"F1 Score: \", mean_f1_score_cnn)\n",
    "print(\"XGB\")\n",
    "print(\"--------------------\")\n",
    "print(\"Precision: \", mean_precision_xgb)\n",
    "print(\"Recall: \", mean_recall_xgb)\n",
    "print(\"F1 Score: \", mean_f1_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_precision_cnn = np.mean([cnn_results.metrics_pp_test.scores_list[x].F1 for x in range(5)])\n",
    "mean_recall_cnn = np.mean([cnn_results.metrics_pp_test.scores_list[x].recall for x in range(5)])\n",
    "mean_f1_score_cnn = np.mean([cnn_results.metrics_pp_test.scores_list[x].precision for x in range(5)])\n",
    "\n",
    "mean_precision_xgb = np.mean([xgb_results.metrics_pp_test.scores_list[x].F1 for x in range(5)])\n",
    "mean_recall_xgb = np.mean([xgb_results.metrics_pp_test.scores_list[x].recall for x in range(5)])\n",
    "mean_f1_score_xgb = np.mean([xgb_results.metrics_pp_test.scores_list[x].precision for x in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "--------------------\n",
      "Precision:  0.961206568381202\n",
      "Recall:  0.9772771792360431\n",
      "F1 Score:  0.9457606414262774\n",
      "XGB\n",
      "--------------------\n",
      "Precision:  0.9603155706528002\n",
      "Recall:  0.9808031341821742\n",
      "F1 Score:  0.9407025566034226\n"
     ]
    }
   ],
   "source": [
    "# print results in a visally appealing way\n",
    "print(\"CNN\")\n",
    "print(\"--------------------\")\n",
    "print(\"Precision: \", mean_precision_cnn)\n",
    "print(\"Recall: \", mean_recall_cnn)\n",
    "print(\"F1 Score: \", mean_f1_score_cnn)\n",
    "print(\"XGB\")\n",
    "print(\"--------------------\")\n",
    "print(\"Precision: \", mean_precision_xgb)\n",
    "print(\"Recall: \", mean_recall_xgb)\n",
    "print(\"F1 Score: \", mean_f1_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9407025566034226"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_f1_score_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1233074, 454176, 300879, 104046, 189164, 159913]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_results.metrics_pp_val.n_samples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F1',\n",
       " 'FN',\n",
       " 'FP',\n",
       " 'IoU',\n",
       " 'RTD_offset',\n",
       " 'RTD_onset',\n",
       " 'RTO_offset',\n",
       " 'RTO_onset',\n",
       " 'TP',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__dataclass_fields__',\n",
       " '__dataclass_params__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__match_args__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'confusion_matrix',\n",
       " 'deletions',\n",
       " 'duration_gt',\n",
       " 'duration_pd',\n",
       " 'insertions',\n",
       " 'mean_IoU',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'replace']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cnn_results.metrics_pp_val.scores_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tom_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
